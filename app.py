{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee6395e6-3904-405c-9f7a-8db8f9b0a0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\luqma\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\luqma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\luqma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\luqma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'random_forest_model.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 262\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jsonify({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhealthy\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 262\u001b[0m     \u001b[43mload_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     app\u001b[38;5;241m.\u001b[39mrun(debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, host\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0.0.0.0\u001b[39m\u001b[38;5;124m'\u001b[39m, port\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPORT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m5000\u001b[39m)))\n",
      "Cell \u001b[1;32mIn[1], line 73\u001b[0m, in \u001b[0;36mload_models\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading models...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Load the main model and vectorizer\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m detector\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrandom_forest_model.joblib\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m detector\u001b[38;5;241m.\u001b[39mtfidf \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidf_vectorizer.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Load the URL extractor\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'random_forest_model.joblib'"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from tld import get_tld\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Initialize with empty components that will be loaded in load_models()\n",
    "class PhishingDetectorAPI:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.tfidf = None\n",
    "        self.url_extractor = None\n",
    "        self.semantic_analyzer_config = None\n",
    "        self.tokenizer = None\n",
    "        self.bert_model = None\n",
    "        self.sentence_model = None\n",
    "\n",
    "# Initialize the detector\n",
    "detector = PhishingDetectorAPI()\n",
    "\n",
    "# Word lists (same as in your notebook)\n",
    "URGENT_WORDS = ['urgent', 'immediately', 'now', 'hurry', 'deadline', 'limited time', 'act now', 'right away', 'expire', 'last chance', 'final notice', 'immediate action required', 'response required', 'time sensitive', 'expiring soon', 'limited offer', 'closing soon', 'don\\'t delay', 'only today', 'last opportunity', 'final warning', 'action needed', 'attention required', 'quick response', 'rush', 'asap', 'right now', 'without delay', 'instant', 'prompt', 'today only']\n",
    "TOO_GOOD_WORDS = ['win', 'won', 'prize', 'award', 'free', 'gift', 'bonus', 'selected', 'congratulations', 'reward', 'jackpot', 'lucky', 'winner', 'giveaway', 'million', 'billion', 'cash', 'fortune', 'wealth', 'rich', 'exclusive', 'vip', 'special offer', 'no cost', 'no fee', 'guaranteed', 'risk-free', 'once in a lifetime', 'life-changing', 'miracle', 'amazing', 'incredible', 'unbelievable', 'limited edition', 'secret', 'hidden', 'exclusive']\n",
    "REQUEST_WORDS = ['verify', 'confirm', 'account', 'password', 'login', 'click', 'update', 'information', 'personal', 'details', 'security', 'credentials', 'validate', 'authenticate', 'renew', 'reactivate', 'unlock', 'suspend', 'restrict', 'compromise', 'hacked', 'breach', 'fraud', 'suspicious', 'activity', 'required', 'necessary', 'mandatory', 'immediately', 'now', 'urgent', 'social security', 'credit card', 'bank account', 'ssn', 'pin', 'dob']\n",
    "\n",
    "def preprocess_text(text):\n",
    "    try:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = re.sub(r'\\s+[a-z]\\s+', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        tokens = text.split()\n",
    "        \n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text}. Error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"Load all the model components\"\"\"\n",
    "    print(\"Loading models...\")\n",
    "    \n",
    "    # Load the main model and vectorizer\n",
    "    detector.model = joblib.load('random_forest_model.joblib')\n",
    "    detector.tfidf = joblib.load('tfidf_vectorizer.joblib')\n",
    "    \n",
    "    # Load the URL extractor\n",
    "    with open('url_extractor.pkl', 'rb') as f:\n",
    "        detector.url_extractor = pickle.load(f)\n",
    "    \n",
    "    # Load the semantic analyzer config\n",
    "    with open('semantic_analyzer_config.pkl', 'rb') as f:\n",
    "        detector.semantic_analyzer_config = pickle.load(f)\n",
    "    \n",
    "    # Initialize BERT and Sentence Transformer models\n",
    "    print(\"Loading BERT and SentenceTransformer models...\")\n",
    "    detector.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    detector.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    detector.sentence_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    \n",
    "    print(\"All models loaded successfully\")\n",
    "\n",
    "class SemanticAnalyzer:\n",
    "    def __init__(self, config):\n",
    "        self.known_phishing_phrases = config['known_phishing_phrases']\n",
    "        self.negative_tone_words = config['negative_tone_words']\n",
    "        self.overly_positive_words = config['overly_positive_words']\n",
    "        \n",
    "        # Initialize embeddings for known phishing phrases\n",
    "        self.phishing_embeddings = detector.sentence_model.encode(self.known_phishing_phrases)\n",
    "    \n",
    "    def analyze_tone(self, text):\n",
    "        negative_score = sum(text.lower().count(word) for word in self.negative_tone_words)\n",
    "        positive_score = sum(text.lower().count(word) for word in self.overly_positive_words)\n",
    "        \n",
    "        if negative_score >= 2 and positive_score == 0:\n",
    "            return \"negative/urgent\"\n",
    "        elif positive_score >= 2 and negative_score == 0:\n",
    "            return \"overly_positive\"\n",
    "        elif positive_score >= 1 and negative_score >= 1:\n",
    "            return \"mixed_emotional_manipulation\"\n",
    "        else:\n",
    "            return \"neutral\"\n",
    "    \n",
    "    def detect_generic_greeting(self, text):\n",
    "        first_sentence = text.split('.')[0].lower() if '.' in text else text.lower()\n",
    "        generic_greetings = ['dear customer', 'dear sir/madam', 'valued customer', 'dear account holder', 'dear member', 'dear user', 'dear client', 'dear winner', 'attention customer', 'account notification', 'service notification', 'important notice', 'security alert', 'dear friend', 'dear valued one', 'dear email user', 'dear account owner', 'dear webmail user', 'dear mail user', 'dear subscriber']\n",
    "        return any(greet in first_sentence for greet in generic_greetings)\n",
    "    \n",
    "    def analyze_semantics(self, text):\n",
    "        inputs = detector.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = detector.bert_model(**inputs)\n",
    "        bert_embedding = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "        \n",
    "        text_embedding = detector.sentence_model.encode([text])\n",
    "        similarities = cosine_similarity(text_embedding, self.phishing_embeddings)\n",
    "        max_similarity = np.max(similarities)\n",
    "        avg_similarity = np.mean(similarities)\n",
    "        \n",
    "        tone = self.analyze_tone(text)\n",
    "        generic_greeting = self.detect_generic_greeting(text)\n",
    "        \n",
    "        blob = TextBlob(text)\n",
    "        spelling_errors = len(blob.correct().split()) - len(blob.split())\n",
    "        \n",
    "        contextual_anomaly = self.detect_contextual_anomalies(text)\n",
    "        \n",
    "        return {\n",
    "            'bert_embedding': bert_embedding,\n",
    "            'max_phishing_similarity': max_similarity,\n",
    "            'avg_phishing_similarity': avg_similarity,\n",
    "            'tone': tone,\n",
    "            'generic_greeting': generic_greeting,\n",
    "            'spelling_errors': spelling_errors,\n",
    "            'contextual_anomalies': contextual_anomaly,\n",
    "            'is_semantically_suspicious': (\n",
    "                max_similarity > 0.75 or \n",
    "                avg_similarity > 0.55 or\n",
    "                tone in [\"negative/urgent\", \"overly_positive\", \"mixed_emotional_manipulation\"] or\n",
    "                generic_greeting or\n",
    "                spelling_errors >= 2 or\n",
    "                contextual_anomaly\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    def detect_contextual_anomalies(self, text):\n",
    "        common_brands = ['paypal', 'bank', 'amazon', 'ebay', 'apple', 'microsoft', 'netflix', 'spotify', 'irs', 'tax', 'dhl', 'fedex', 'ups']\n",
    "        brand_in_text = any(brand in text.lower() for brand in common_brands)\n",
    "        suspicious_context = any(word in text.lower() for word in ['click', 'verify', 'update', 'confirm'])\n",
    "        return brand_in_text and suspicious_context\n",
    "\n",
    "def predict_message(message):\n",
    "    \"\"\"Predict if a message is phishing with detailed explanations\"\"\"\n",
    "    if not detector.model:\n",
    "        raise ValueError(\"Models not loaded\")\n",
    "    \n",
    "    # Initialize semantic analyzer\n",
    "    semantic_analyzer = SemanticAnalyzer(detector.semantic_analyzer_config)\n",
    "    \n",
    "    # Preprocess text\n",
    "    processed_text = preprocess_text(message)\n",
    "    \n",
    "    # Extract features\n",
    "    features = {}\n",
    "    \n",
    "    # 1. Traditional features\n",
    "    features['urgent_count'] = sum(message.lower().count(word) for word in URGENT_WORDS)\n",
    "    features['too_good_count'] = sum(message.lower().count(word) for word in TOO_GOOD_WORDS)\n",
    "    features['request_count'] = sum(message.lower().count(word) for word in REQUEST_WORDS)\n",
    "    features['message_length'] = len(message)\n",
    "    features['special_chars'] = len(re.findall(r'[^\\w\\s]', message))\n",
    "    features['caps_words'] = len(re.findall(r'\\b[A-Z]{2,}\\b', message))\n",
    "    features['phone_numbers'] = len(re.findall(r'[\\+\\(]?[0-9][0-9\\-\\(\\)]{8,}[0-9]', message))\n",
    "    \n",
    "    # URL features\n",
    "    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', message)\n",
    "    features['has_url'] = int(len(urls) > 0)\n",
    "    \n",
    "    url_suspicion_scores = []\n",
    "    url_predictions = []\n",
    "    \n",
    "    for url in urls:\n",
    "        url_feat = detector.url_extractor.extract_url_features(url)\n",
    "        prediction = detector.url_extractor.predict_url(url_feat)\n",
    "        url_predictions.append(prediction)\n",
    "        url_suspicion_scores.append(prediction['probability'])\n",
    "    \n",
    "    features['max_url_suspicion'] = max(url_suspicion_scores) if url_suspicion_scores else 0\n",
    "    features['avg_url_suspicion'] = np.mean(url_suspicion_scores) if url_suspicion_scores else 0\n",
    "    features['phishing_url_count'] = sum(1 for pred in url_predictions if pred['prediction'] == 1)\n",
    "    \n",
    "    # 2. Semantic features\n",
    "    semantic_features = semantic_analyzer.analyze_semantics(message)\n",
    "    \n",
    "    features['max_phishing_similarity'] = semantic_features['max_phishing_similarity']\n",
    "    features['avg_phishing_similarity'] = semantic_features['avg_phishing_similarity']\n",
    "    features['tone_negative'] = int(semantic_features['tone'] == \"negative/urgent\")\n",
    "    features['tone_positive'] = int(semantic_features['tone'] == \"overly_positive\")\n",
    "    features['tone_mixed'] = int(semantic_features['tone'] == \"mixed_emotional_manipulation\")\n",
    "    features['generic_greeting'] = int(semantic_features['generic_greeting'])\n",
    "    features['spelling_errors'] = semantic_features['spelling_errors']\n",
    "    features['contextual_anomaly'] = int(semantic_features['contextual_anomalies'])\n",
    "    \n",
    "    features_df = pd.DataFrame([features])\n",
    "    \n",
    "    # Create TF-IDF features\n",
    "    tfidf_features = detector.tfidf.transform([processed_text])\n",
    "    tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=detector.tfidf.get_feature_names_out())\n",
    "    \n",
    "    # Combine features\n",
    "    X = pd.concat([features_df.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    # Predict\n",
    "    probability = detector.model.predict_proba(X)[0][1]\n",
    "    is_phishing = probability > 0.5\n",
    "    \n",
    "    return {\n",
    "        'is_phishing': bool(is_phishing),\n",
    "        'probability': float(probability),\n",
    "        'features': features,\n",
    "        'url_predictions': url_predictions,\n",
    "        'semantic_features': semantic_features\n",
    "    }\n",
    "\n",
    "# API Endpoints\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json()\n",
    "    if not data or 'message' not in data:\n",
    "        return jsonify({'error': 'No message provided'}), 400\n",
    "    \n",
    "    try:\n",
    "        result = predict_message(data['message'])\n",
    "        return jsonify({\n",
    "            'status': 'success',\n",
    "            'result': {\n",
    "                'is_phishing': result['is_phishing'],\n",
    "                'probability': result['probability'],\n",
    "                'features': result['features'],\n",
    "                'url_predictions': result['url_predictions'],\n",
    "                'semantic_features': result['semantic_features']\n",
    "            }\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    return jsonify({'status': 'healthy'})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    load_models()\n",
    "    app.run(debug=True, host='0.0.0.0', port=int(os.environ.get('PORT', 5000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff40755-aa62-40b2-aedc-d8fdd71e7921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
